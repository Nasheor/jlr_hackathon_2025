{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-12T16:46:20.905477Z",
     "start_time": "2025-06-12T01:56:22.883625Z"
    }
   },
   "source": [
    " # %% [markdown] -------------------------------------------------------------\n",
    "# # Task 4 – Fine-tune binary vulnerability classifier\n",
    "# • Base model : microsoft/codebert-base (110 M)\n",
    "# • Adapter   : LoRA  (rank = 8)\n",
    "# • Output    : models/codebert_mini_lora/  +  metrics.json\n",
    "# Expected run-time on RTX-3060: ≈ 45 min (5 epochs, batch 8, fp16)\n",
    "\n",
    "# %% [code] ▸ 0 Imports & paths\n",
    "import pathlib, time, json, torch, evaluate\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
    "                          TrainingArguments, Trainer, DataCollatorWithPadding)\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from src.train_classifier_utils import seed_everything, load_splits\n",
    "from transformers import DataCollatorWithPadding\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "\n",
    "\n",
    "TRAIN_JSONL = \"../data/splits/train.jsonl\"\n",
    "VALID_JSONL = \"../data/splits/valid.jsonl\"\n",
    "OUT_DIR     = pathlib.Path(\"../models/codebert_mini_lora\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "seed_everything(42)\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "# %% [code] ▸ 1 Load splits\n",
    "ds = load_splits(TRAIN_JSONL, VALID_JSONL)\n",
    "print(ds)\n",
    "\n",
    "# %% [code] ▸ 2 Tokeniser & map\n",
    "tok = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tok, padding=\"longest\")\n",
    "MAX_LEN = 512\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tok(batch[\"Function before\"],\n",
    "               truncation=True, max_length=MAX_LEN, padding=\"max_length\")\n",
    "\n",
    "ds = ds.map(tokenize, batched=True, remove_columns=[\"Function before\"])\n",
    "ds = ds.rename_column(\"is_vuln\", \"labels\")\n",
    "ds.set_format(type=\"torch\")\n",
    "\n",
    "# %% [code] ▸ 3 Base model + LoRA adapter\n",
    "base = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"microsoft/codebert-base\", num_labels=2\n",
    ")\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    r=8, lora_alpha=16, lora_dropout=0.05,\n",
    "    target_modules=[\"query\",\"value\"],  # attention layers\n",
    "    bias=\"none\", task_type=\"SEQ_CLS\"\n",
    ")\n",
    "model = get_peft_model(base, lora_cfg).to(DEVICE)\n",
    "# compute class imbalance on the train split\n",
    "labels = ds[\"train\"][\"labels\"]\n",
    "pos = int((labels == 1).sum())   # number of vulnerable samples\n",
    "neg = int((labels == 0).sum())   # number of clean samples\n",
    "clean_weight = pos / neg if neg else 1.0        # >1 if many vulns\n",
    "loss_weights = torch.tensor([clean_weight, 1.0]).to(DEVICE)\n",
    "\n",
    "# patch into the model (PyTorch CROssEntropyLoss supports weight=…)\n",
    "model.classifier.loss_fct = CrossEntropyLoss(weight=loss_weights)\n",
    "print(f\"Class weights → clean: {clean_weight:.2f}   vuln: 1.0\")\n",
    "\n",
    "model.print_trainable_parameters()    # sanity: ~0.5 M params trainable\n",
    "\n",
    "# %% [code] ▸ 4 Metric callback\n",
    "sk_metrics = evaluate.combine([\"precision\", \"recall\", \"f1\"])\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    return sk_metrics.compute(predictions=preds, references=labels)\n",
    "\n",
    "# be sure to import evaluate earlier:\n",
    "#   import evaluate\n",
    "#   sk_metrics = evaluate.combine([\"precision\", \"recall\", \"f1\"])\n",
    "\n",
    "# %% [code] ▸ 5 HF Trainer setup\n",
    "args = TrainingArguments(\n",
    "    output_dir=str(OUT_DIR),\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=2e-4,\n",
    "    warmup_ratio=0.05,\n",
    "    fp16=(DEVICE==\"cuda\"),\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# data_collator = DataCollatorWithPadding(tokenizer=tok, padding=\"longest\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "t0 = time.time()\n",
    "trainer.train()\n",
    "print(f\"⏱ Finished in {(time.time()-t0)/60:.1f} min\")\n",
    "\n",
    "# %% [code] ▸ 6 Save weights & metrics\n",
    "trainer.save_model(str(OUT_DIR))            # saves only LoRA adapter\n",
    "metrics = trainer.evaluate(ds[\"validation\"])\n",
    "with open(OUT_DIR / \"metrics.json\", \"w\") as w:\n",
    "    json.dump(metrics, w, indent=2)\n",
    "metrics\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Function before', 'is_vuln', 'cvss'],\n",
      "        num_rows: 563\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['Function before', 'is_vuln', 'cvss'],\n",
      "        num_rows: 70\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 70/70 [00:00<00:00, 795.28 examples/s]\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights → clean: 1.00   vuln: 1.0\n",
      "trainable params: 887,042 || all params: 125,534,212 || trainable%: 0.7066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "C:\\Users\\nagar\\anaconda3\\envs\\hackathon_2025\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='213' max='213' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [213/213 47:50, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.748600</td>\n",
       "      <td>0.696745</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.715300</td>\n",
       "      <td>0.693790</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.719200</td>\n",
       "      <td>0.692939</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.243902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nagar\\anaconda3\\envs\\hackathon_2025\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\nagar\\anaconda3\\envs\\hackathon_2025\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\nagar\\anaconda3\\envs\\hackathon_2025\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱ Finished in 48.1 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nagar\\anaconda3\\envs\\hackathon_2025\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [600/600 2:31:56, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6929389834403992,\n",
       " 'eval_precision': 0.8333333333333334,\n",
       " 'eval_recall': 0.14285714285714285,\n",
       " 'eval_f1': 0.24390243902439024,\n",
       " 'eval_runtime': 22.0597,\n",
       " 'eval_samples_per_second': 3.173,\n",
       " 'eval_steps_per_second': 0.408,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nagar\\anaconda3\\envs\\hackathon_2025\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\nagar\\anaconda3\\envs\\hackathon_2025\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱ Finished in 152.2 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nagar\\anaconda3\\envs\\hackathon_2025\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:56]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 6.17799651081441e-06,\n",
       " 'eval_precision': 1.0,\n",
       " 'eval_recall': 1.0,\n",
       " 'eval_f1': 1.0,\n",
       " 'eval_runtime': 59.7813,\n",
       " 'eval_samples_per_second': 3.346,\n",
       " 'eval_steps_per_second': 0.418,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import TrainingArguments\n",
    "TrainingArguments(\n",
    "    output_dir=\"tmp\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=1\n",
    ")\n"
   ],
   "id": "35ced5349c62331b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
