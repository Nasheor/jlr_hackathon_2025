{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-12T11:15:59.437111Z",
     "start_time": "2025-06-12T11:15:51.056786Z"
    }
   },
   "source": [
    "# %% [markdown]\n",
    "### Task 6 – Merge LoRA, export ONNX, INT8-quantise\n",
    "# Run the helper script.\n",
    "# Sanity-check fp32 vs INT8 outputs differ < 1 % on a random batch.\n",
    "\n",
    "# %% [code] ▸ 1  Run the export script\n",
    "# !python ../src/export_quant.py --lora_dir models/codebert_mini_lora --out_dir  models/quantised\n",
    "\n",
    "# %% [code] ▸ 2  Quick functional test\n",
    "import numpy as np, onnxruntime as ort, torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "tok = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "text = \"char buf[8]; memcpy(buf, input, len);\"          # dummy code\n",
    "ids  = tok(text, return_tensors=\"pt\")\n",
    "\n",
    "# fp32 HF model (merged)\n",
    "hf = AutoModelForSequenceClassification.from_pretrained(\"../models/quantised/fp32\")\n",
    "hf.eval()\n",
    "with torch.no_grad():\n",
    "    logits_fp32 = hf(**{k:v for k,v in ids.items()})[0].softmax(-1)[0,1].item()\n",
    "\n",
    "# INT8 ONNX\n",
    "sess = ort.InferenceSession(\"../models/quantised/codebert_int8.onnx\",\n",
    "                            providers=[\"CPUExecutionProvider\"])\n",
    "# inputs = {\n",
    "#     \"input_ids\":       ids[\"input_ids\"].numpy(),\n",
    "#     \"attention_mask\":  ids[\"attention_mask\"].numpy(),\n",
    "#     \"token_type_ids\":  torch.zeros_like(ids[\"input_ids\"]).numpy()  # ← NEW\n",
    "# }\n",
    "inputs = {\n",
    "    \"input_ids\":      ids[\"input_ids\"].numpy(),\n",
    "    \"attention_mask\": ids[\"attention_mask\"].numpy(),\n",
    "    \"token_type_ids\": torch.zeros_like(ids[\"input_ids\"]).numpy()   # all-zeros feed\n",
    "}\n",
    "\n",
    "\n",
    "logits_int8 = sess.run(None, inputs)[0]\n",
    "prob_int8   = torch.softmax(torch.tensor(logits_int8), -1)[0,1].item()\n",
    "\n",
    "print(f\"fp32 prob={logits_fp32:.4f}   int8 prob={prob_int8:.4f}   Δ={abs(prob_int8-logits_fp32):.4f}\")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nagar\\anaconda3\\envs\\hackathon_2025\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fp32 prob=0.4631   int8 prob=0.4474   Δ=0.0157\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
